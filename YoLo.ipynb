{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YoLo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishkhareiitr/projects/blob/master/YoLo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3fryPHCyKkSZ",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "8a6c38e4-d5dd-40ff-f83c-03b7b754ac34"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9bafbe17-f357-402d-a020-0a4baa6548a5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9bafbe17-f357-402d-a020-0a4baa6548a5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.zip to data.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0lnV65yKrOG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import io\n",
        "data = zipfile.ZipFile(io.BytesIO(uploaded['data.zip']), 'r')\n",
        "data.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tXS0Sl1_K2th",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5564
        },
        "outputId": "69bd1744-c2f8-4bc5-b307-74ab50bc87ec"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "_nsre = re.compile('([0-9]+)')\n",
        "def natural_sort_key(s):\n",
        "    return [int(text) if text.isdigit() else text.lower()\n",
        "            for text in re.split(_nsre, s)]     \n",
        "\n",
        "\n",
        "def loadImages(path):\n",
        "    # return array of images\n",
        "    \n",
        "    imagesList = listdir(path)\n",
        "    imagesList.sort(key=natural_sort_key)\n",
        "    #imagesList = imagesList.sort()     \n",
        "    loadedImages = []\n",
        "    for image in imagesList:\n",
        "      if image is not None:\n",
        "        img = PIL.Image.open(path + image)\n",
        "        loadedImages.append(img)\n",
        "      else:\n",
        "        print('not loaded')\n",
        "    \n",
        "    \n",
        "    \n",
        "    return loadedImages\n",
        "\n",
        "\n",
        "# Specify the paths for the 2 files\n",
        "#protoFile = \"A://ISB//Capstone//PoseRetreival//OpenCVPoseEsti//pose_deploy_linevec.prototxt\"\n",
        "protoFile = \"pose_deploy_linevec_faster_4_stages.prototxt\"\n",
        "weightsFile = \"pose_iter_440000.caffemodel\"\n",
        " \n",
        "path_loadImages =  \"stickpose/\"\n",
        "imgs = loadImages(path_loadImages)\n",
        "\n",
        "\n",
        "#path_poseImages = r\"A://ISB//Capstone//PoseRetreival//OpenCVPoseEsti//poseImages//\"\n",
        "\n",
        "# Read the network into Memory\n",
        "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
        "\n",
        "## GET FRAMES\n",
        "#pathOut = r\"A:/ISB/Capstone/PoseRetreival/FramePerSecond/\"\n",
        "#url = 'https://www.youtube.com/watch?v=AbPufvvYiSw'\n",
        "#url='https://www.youtube.com/watch?v=SCr5mEl9qbQ'\n",
        "#vPafy = pafy.new(url)\n",
        "#play = vPafy.getbest(preftype=\"webm\")\n",
        "\n",
        "#Get from the video from local folder\n",
        "vidcap = cv2.VideoCapture(\"yoga3.mp4\")\n",
        "\n",
        "\n",
        "#Get from the video from youtube\n",
        "#vidcap = cv2.VideoCapture(play.url)\n",
        "\n",
        "\n",
        "count = 0\n",
        "success = True\n",
        "'''\n",
        "while success:\n",
        "    success,image = vidcap.read()\n",
        "    print('read a new frame:',success)\n",
        "    if count%90 == 0 :\n",
        "     #    cv2.imshow('test',image)\n",
        "     #    cv2.imwrite(pathOut + 'frame%d.jpg'%count,image)\n",
        "     #    #cv2.waitKey(1000)\n",
        "     #    #cv2.destroyAllWindows()\n",
        "    #count+=1\n",
        "'''    \n",
        "\n",
        "\n",
        "    \n",
        "        \n",
        "for i in range(1,len(imgs)): \n",
        "        frame=cv2.imread(imgs[i-1].filename)\n",
        "        \n",
        "        # Read image\n",
        "        #frame = cv2.imread(\"A://ISB//Capstone//PoseRetreival//OpenCVPoseEsti//Image//Image3.jpg\")\n",
        "        \n",
        "        #frame = image\n",
        "        ###########Resize the image##################\n",
        "         \n",
        "        '''\n",
        "        #basewidth = 1280\n",
        "        #img = PIL.Image.open(\"A://ISB//Capstone//PoseRetreival//OpenCVPoseEsti//Image//Image3.jpg\")\n",
        "        width, height = img.size\n",
        "        #print(width,height)\n",
        "         \n",
        "        wpercent = (basewidth/float(img.size[0]))\n",
        "        hsize = int((float(img.size[1])*float(wpercent)))\n",
        "        img = img.resize((basewidth,hsize), PIL.Image.ANTIALIAS)\n",
        "        '''\n",
        "        #width, height = frame.size\n",
        "        height, width, channels = frame.shape\n",
        "        print(width,height)\n",
        "        #img.save('sompic.jpg') \n",
        "        if width <600:\n",
        "            x=width\n",
        "            y= 600 -x\n",
        "            width = width +x\n",
        "            height =  height +int((height/x)*y)\n",
        "            frame = cv2.resize(frame, (width, height)) \n",
        "       \n",
        "        ################################################\n",
        "        # Read image\n",
        "        #frame = cv2.imread(imgs[i-1].filename) #cv2.imread(\"sompic.jpg\")\n",
        "         \n",
        "        # Specify the input image dimensions\n",
        "        inWidth = width +0\n",
        "        inHeight = height + int((height/width)*0)\n",
        "        #print(inWidth,inHeight)\n",
        "        # Prepare the frame to be fed to the network\n",
        "        inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n",
        "         \n",
        "        # Set the prepared object as the input blob of the network\n",
        "        net.setInput(inpBlob)\n",
        "        \n",
        "        p=((0,1),(2,5),(3,6),(4,7),(8,11),(9,12),(10,13),(1,8),(1,11))        \n",
        "        output = net.forward()\n",
        "        \n",
        "        H = output.shape[2]\n",
        "        W = output.shape[3]\n",
        "        # Empty list to store the detected keypoints\n",
        "        points = {}\n",
        "        for i in range(0,13):\n",
        "            # confidence map of corresponding body's part.\n",
        "            probMap = output[0, i, :, :]\n",
        "        \n",
        "            # Find global maxima of the probMap.\n",
        "            minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
        "             \n",
        "            # Scale the point to fit on the original image\n",
        "            #x = (frameWidth * point[0]) / W\n",
        "            #y = (frameHeight * point[1]) / H\n",
        "            x = (inWidth * point[0]) / W\n",
        "            y = (inHeight * point[1]) / H\n",
        "            if prob > 0.01 : \n",
        "                cv2.circle(frame, (int(x), int(y)), 15, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
        "                cv2.putText(frame, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 255), 3, lineType=cv2.LINE_AA)\n",
        "         \n",
        "                # Add the point to the list if the probability is greater than the threshold\n",
        "                points.update({i:(int(x), int(y))})\n",
        "                #print(prob)\n",
        "            else :\n",
        "                #points.update(None)\n",
        "                 print(points)\n",
        "        \n",
        "        for pair in p:\n",
        "            partA = pair[0]\n",
        "            partB = pair[1]\n",
        "            print(partA,partB)\n",
        "            #if points[partA] and points[partB]:\n",
        "            if (partA in points) and (partB in points):\n",
        "                cv2.line(frame, points[partA], points[partB], (0, 255, 0), 3)    \n",
        "        \n",
        "        cv2.imwrite(os.path.join(os.getcwd(),\"stickpose{:d}.jpg\".format(count)),frame )         \n",
        "        #cv2.imshow(\"Output-Keypoints\",frame)\n",
        "        #cv2.waitKey(5000)\n",
        "        #cv2.destroyAllWindows()\n",
        "        count+=1"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "474 327\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "480 480\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "500 500\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1000 667\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1080 1108\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "480 480\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "960 720\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "500 333\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "640 640\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "720 810\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "570 310\n",
            "{0: (310, 55), 1: (350, 111), 2: (406, 103), 3: (223, 190), 4: (167, 270), 5: (310, 119), 6: (223, 214), 7: (167, 270), 8: (374, 238), 9: (231, 87)}\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "500 500\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "400 300\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "768 1024\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "400 500\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "450 360\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "640 360\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1280 720\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1280 720\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1280 720\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "640 360\n",
            "{0: (344, 96), 1: (344, 184), 2: (272, 184), 3: (240, 336), 4: (320, 272), 5: (424, 192), 6: (448, 336), 7: (368, 280), 8: (320, 352)}\n",
            "{0: (344, 96), 1: (344, 184), 2: (272, 184), 3: (240, 336), 4: (320, 272), 5: (424, 192), 6: (448, 336), 7: (368, 280), 8: (320, 352)}\n",
            "{0: (344, 96), 1: (344, 184), 2: (272, 184), 3: (240, 336), 4: (320, 272), 5: (424, 192), 6: (448, 336), 7: (368, 280), 8: (320, 352)}\n",
            "{0: (344, 96), 1: (344, 184), 2: (272, 184), 3: (240, 336), 4: (320, 272), 5: (424, 192), 6: (448, 336), 7: (368, 280), 8: (320, 352)}\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "860 645\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "500 400\n",
            "{0: (368, 136), 1: (368, 176), 2: (336, 168), 3: (344, 272), 4: (384, 304), 5: (400, 192), 6: (344, 264), 7: (384, 312), 8: (392, 296), 9: (344, 360)}\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "159 240\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "600 450\n",
            "{0: (312, 228), 1: (320, 78), 2: (328, 78), 3: (296, 142), 4: (328, 173), 5: (312, 86), 6: (304, 142), 7: (328, 173), 8: (320, 181), 9: (344, 205)}\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1300 1173\n",
            "{0: (765, 303), 1: (765, 319), 2: (693, 295), 3: (574, 438), 4: (693, 383), 5: (893, 582), 6: (582, 438), 7: (622, 414), 8: (845, 670), 9: (829, 686)}\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1280 720\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "496 368\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1280 720\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1280 720\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "1280 720\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n",
            "640 640\n",
            "{0: (312, 136), 1: (296, 176), 2: (288, 168), 3: (280, 328), 4: (320, 288), 5: (280, 304), 6: (320, 416), 7: (320, 288), 8: (328, 304), 9: (320, 368)}\n",
            "0 1\n",
            "2 5\n",
            "3 6\n",
            "4 7\n",
            "8 11\n",
            "9 12\n",
            "10 13\n",
            "1 8\n",
            "1 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zLjFIT-eiU5Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "b67212b6-c6a1-4faf-e669-4728a8dd7abf"
      },
      "cell_type": "code",
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-11 11:31:46--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.3.39\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.3.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M   108MB/s    in 2.2s    \n",
            "\n",
            "2018-11-11 11:31:48 (108 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b4z48LzvhxAD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Object detection - YOLO - OpenCV\n",
        "# Author : Arun Ponnusamy   (July 16, 2018)\n",
        "# Website : http://www.arunponnusamy.com\n",
        "############################################\n",
        "\n",
        "\n",
        "import cv2\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "import PIL.Image\n",
        "\n",
        "import os\n",
        "from sys import platform\n",
        "import re\n",
        "from os import listdir\n",
        "import ntpath #for retrieving filename\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#ap = argparse.ArgumentParser()\n",
        "#ap.add_argument('-i', '--image', required=True,\n",
        "#                help = 'path to input image')\n",
        "#ap.add_argument('-c', '--config', required=True,\n",
        "#                help = 'path to yolo config file')\n",
        "#ap.add_argument('-w', '--weights', required=True,\n",
        "#                help = 'path to yolo pre-trained weights')\n",
        "#ap.add_argument('-cl', '--classes', required=True,\n",
        "#                help = 'path to text file containing class names')\n",
        "#args = ap.parse_args()\n",
        "\n",
        "\n",
        "def get_output_layers(net):\n",
        "    \n",
        "    layer_names = net.getLayerNames()\n",
        "    \n",
        "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "    return output_layers\n",
        "\n",
        "\n",
        "def draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
        "\n",
        "    label = str(classes[class_id])\n",
        "\n",
        "    color = COLORS[class_id]\n",
        "\n",
        "    image = cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
        "\n",
        "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "    \n",
        "    return image\n",
        "\n",
        "    \n",
        "_nsre = re.compile('([0-9]+)')\n",
        "def natural_sort_key(s):\n",
        "    return [int(text) if text.isdigit() else text.lower()\n",
        "            for text in re.split(_nsre, s)]     \n",
        "def loadImages(path):\n",
        "    # return array of images\n",
        "    \n",
        "    imagesList = listdir(path)\n",
        "    imagesList.sort(key=natural_sort_key)\n",
        "    #imagesList = imagesList.sort()     \n",
        "    loadedImages = []\n",
        "    for image in imagesList:\n",
        "      if image is not None:\n",
        "        img = PIL.Image.open(path + image)\n",
        "        loadedImages.append(img)\n",
        "      else:\n",
        "        print('not loaded')\n",
        "    \n",
        "       \n",
        "    return loadedImages\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHIbnebdjGF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1562
        },
        "outputId": "6bdfee10-b288-4759-97f8-9b9555eaa4e5"
      },
      "cell_type": "code",
      "source": [
        "path_loadImages =  \"backup data/train/8/\"\n",
        "count = 0\n",
        "imgs = loadImages(path_loadImages)\n",
        "\n",
        "for i in range(1,len(imgs)):\n",
        "  \n",
        "  print(imgs[i-1].filename)\n",
        "\n",
        "  image=cv2.imread(imgs[i-1].filename)\n",
        "\n",
        "#image = cv2.imread(\"frame328.jpg\")\n",
        "\n",
        "  Width = image.shape[1]\n",
        "  Height = image.shape[0]\n",
        "  scale = 0.00392\n",
        "\n",
        "  classes = None\n",
        "\n",
        "  with open(\"yolov3.txt\", 'r') as f:\n",
        "    \n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "\n",
        "  weights = \"yolov3.weights\"\n",
        "  config = \"yolov3.cfg\"\n",
        "\n",
        "  net = cv2.dnn.readNet(weights, config)\n",
        "\n",
        "  blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
        "\n",
        "  net.setInput(blob)\n",
        "\n",
        "  outs = net.forward(get_output_layers(net))\n",
        "\n",
        "  class_ids = []\n",
        "  confidences = []\n",
        "  boxes = []\n",
        "  conf_threshold = 0.5\n",
        "  nms_threshold = 0.4\n",
        "\n",
        "\n",
        "  for out in outs:\n",
        "      for detection in out:\n",
        "          scores = detection[5:]\n",
        "          class_id = np.argmax(scores)\n",
        "          confidence = scores[class_id]\n",
        "          if confidence > 0.5:\n",
        "              center_x = int(detection[0] * Width)\n",
        "              center_y = int(detection[1] * Height)\n",
        "              w = int(detection[2] * Width)\n",
        "              h = int(detection[3] * Height)\n",
        "              x = center_x - w / 2\n",
        "              y = center_y - h / 2\n",
        "              class_ids.append(class_id)\n",
        "              confidences.append(float(confidence))\n",
        "              boxes.append([x, y, w, h])\n",
        "\n",
        "\n",
        "  indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
        "\n",
        "  for i in indices:\n",
        "      i = i[0]\n",
        "      box = boxes[i]\n",
        "      x = box[0]\n",
        "      y = box[1]\n",
        "      w = box[2]\n",
        "      h = box[3]\n",
        "      draw_prediction(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
        "\n",
        "  #cv2.imshow(\"object detection\", image)\n",
        "  #cv2.waitKey()\n",
        "\n",
        "  #cv2.imwrite(\"object-detection.jpg\", image)\n",
        "  #cv2.destroyAllWindows()\n",
        "  cv2.imwrite(os.path.join(os.getcwd(),\"_8yolo{:d}.jpg\".format(count)),image )\n",
        "  \n",
        "  xx = int(x)\n",
        "  yy = int(y)\n",
        "  ww = int(w)\n",
        "  hh = int(h)\n",
        "  \n",
        "  img = cv2.imread(\"_8yolo{:d}.jpg\".format(count))\n",
        "  print(xx,yy,ww,hh)\n",
        "  \n",
        "  if img is not None:\n",
        "    \n",
        "    \n",
        "    crop_img = img[yy:yy+hh,xx:xx+ww]\n",
        "  #crop_img = img[604:700,420:900]\n",
        "  #y:y+h, x:x+w]\n",
        "  #cv2.imshow(\"cropped\", crop_img)\n",
        "    cv2.imwrite(os.path.join(os.getcwd(),\"_8cropyolo{:d}.jpg\".format(count)),crop_img)\n",
        "  else:\n",
        "    print(\"not cropped\")\n",
        "  \n",
        "  count = count +1\n",
        "  \n",
        "  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "backup data/train/8/500_F_175054143_TMoVXg7KGOHSiPlMfIYN2Gk0Ldty9S2T.jpg\n",
            "15 40 178 116\n",
            "backup data/train/8/frame4.jpg\n",
            "105 56 30 198\n",
            "backup data/train/8/frame5.jpg\n",
            "3 -11 196 214\n",
            "backup data/train/8/frame6.jpg\n",
            "3 -10 197 212\n",
            "backup data/train/8/frame44.jpg\n",
            "254 30 70 307\n",
            "backup data/train/8/frame45.jpg\n",
            "255 32 70 305\n",
            "backup data/train/8/frame46.jpg\n",
            "255 31 70 307\n",
            "backup data/train/8/frame47.jpg\n",
            "254 31 71 312\n",
            "backup data/train/8/frame155.jpg\n",
            "394 129 66 175\n",
            "backup data/train/8/frame232.jpg\n",
            "366 35 78 280\n",
            "backup data/train/8/frame233.jpg\n",
            "366 40 78 270\n",
            "backup data/train/8/frame292.jpg\n",
            "163 39 108 287\n",
            "backup data/train/8/frame293.jpg\n",
            "162 39 109 287\n",
            "backup data/train/8/frame294.jpg\n",
            "162 39 109 287\n",
            "backup data/train/8/frame425.jpg\n",
            "135 332 41 44\n",
            "backup data/train/8/frame436.jpg\n",
            "135 388 39 44\n",
            "backup data/train/8/frame437.jpg\n",
            "137 392 37 42\n",
            "backup data/train/8/frame438.jpg\n",
            "136 397 39 43\n",
            "backup data/train/8/frame641.jpg\n",
            "706 30 208 689\n",
            "backup data/train/8/frame642.jpg\n",
            "704 33 208 683\n",
            "backup data/train/8/frame643.jpg\n",
            "702 36 209 681\n",
            "backup data/train/8/frame840.jpg\n",
            "300 9 162 679\n",
            "backup data/train/8/frame847.jpg\n",
            "281 -26 163 798\n",
            "backup data/train/8/frame849.jpg\n",
            "285 -47 150 892\n",
            "backup data/train/8/frame878.jpg\n",
            "373 42 74 283\n",
            "backup data/train/8/frame949.jpg\n",
            "471 35 305 636\n",
            "backup data/train/8/frame950.jpg\n",
            "472 42 314 627\n",
            "backup data/train/8/frame951.jpg\n",
            "467 33 322 647\n",
            "backup data/train/8/frame952.jpg\n",
            "469 38 323 637\n",
            "backup data/train/8/frame1172.jpg\n",
            "469 38 323 637\n",
            "backup data/train/8/frame1173.jpg\n",
            "469 38 323 637\n",
            "backup data/train/8/frame1174.jpg\n",
            "469 38 323 637\n",
            "backup data/train/8/frame1180.jpg\n",
            "469 38 323 637\n",
            "backup data/train/8/frame1194.jpg\n",
            "235 170 37 50\n",
            "backup data/train/8/frame1195.jpg\n",
            "235 170 37 52\n",
            "backup data/train/8/frame1196.jpg\n",
            "236 170 37 50\n",
            "backup data/train/8/frame1202.jpg\n",
            "236 170 37 50\n",
            "backup data/train/8/frame1203.jpg\n",
            "363 42 31 64\n",
            "backup data/train/8/frame1204.jpg\n",
            "306 47 126 252\n",
            "backup data/train/8/frame1205.jpg\n",
            "191 186 54 68\n",
            "backup data/train/8/images2IWMO20K.jpg\n",
            "92 28 38 191\n",
            "backup data/train/8/images616I0LG8.jpg\n",
            "92 28 38 191\n",
            "backup data/train/8/imagesELG0T60R.jpg\n",
            "116 12 27 165\n",
            "backup data/train/8/imagesL0A5SEF8.jpg\n",
            "44 157 206 12\n",
            "backup data/train/8/imagesL7XMDE82.jpg\n",
            "88 -9 39 243\n",
            "backup data/train/8/imagesTPTAY0EB.jpg\n",
            "49 17 75 239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zkNA38bd9SsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6b04daf5-d3dd-4d2e-c05f-7ca0253ccc3f"
      },
      "cell_type": "code",
      "source": [
        "  import cv2\n",
        "  cnt = 0\n",
        "  img = cv2.imread(\"_2yolo{:d}.jpg\".format(cnt)))\n",
        "  print(x,y,w,h)\n",
        "  \n",
        "  if img is not None:\n",
        "    \n",
        "    \n",
        "    crop_img = img[int(y):int(y+h),int(x):int(x+w)]\n",
        "  #crop_img = img[604:700,420:900]\n",
        "  #y:y+h, x:x+w]\n",
        "  #cv2.imshow(\"cropped\", crop_img)\n",
        "    cv2.imwrite(os.path.join(os.getcwd(),\"_2cropyolo{:d}.jpg\".format(count)),crop_img)\n",
        "  else:\n",
        "    print(\"not cropped\")\n",
        "    \n",
        "   cnt = cnt + 1\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "285.5 354.0 439 354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}